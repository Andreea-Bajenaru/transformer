{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget, os, gzip, pickle, random, re, sys\n",
    "\n",
    "IMDB_URL = 'http://dlvu.github.io/data/imdb.{}.pkl.gz'\n",
    "IMDB_FILE = 'imdb.{}.pkl.gz'\n",
    "\n",
    "PAD, START, END, UNK = '.pad', '.start', '.end', '.unk'\n",
    "\n",
    "def load_imdb(final=False, val=5000, seed=0, voc=None, char=False):\n",
    "\n",
    "    cst = 'char' if char else 'word'\n",
    "\n",
    "    imdb_url = IMDB_URL.format(cst)\n",
    "    imdb_file = IMDB_FILE.format(cst)\n",
    "\n",
    "    if not os.path.exists(imdb_file):\n",
    "        wget.download(imdb_url)\n",
    "\n",
    "    with gzip.open(imdb_file) as file:\n",
    "        sequences, labels, i2w, w2i = pickle.load(file)\n",
    "\n",
    "    if voc is not None and voc < len(i2w):\n",
    "        nw_sequences = {}\n",
    "\n",
    "        i2w = i2w[:voc]\n",
    "        w2i = {w: i for i, w in enumerate(i2w)}\n",
    "\n",
    "        mx, unk = voc, w2i['.unk']\n",
    "        for key, seqs in sequences.items():\n",
    "            nw_sequences[key] = []\n",
    "            for seq in seqs:\n",
    "                seq = [s if s < mx else unk for s in seq]\n",
    "                nw_sequences[key].append(seq)\n",
    "\n",
    "        sequences = nw_sequences\n",
    "\n",
    "    if final:\n",
    "        return (sequences['train'], labels['train']), (sequences['test'], labels['test']), (i2w, w2i), 2\n",
    "\n",
    "    # Make a validation split\n",
    "    random.seed(seed)\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    x_val, y_val = [], []\n",
    "\n",
    "    val_ind = set( random.sample(range(len(sequences['train'])), k=val) )\n",
    "    for i, (s, l) in enumerate(zip(sequences['train'], labels['train'])):\n",
    "        if i in val_ind:\n",
    "            x_val.append(s)\n",
    "            y_val.append(l)\n",
    "        else:\n",
    "            x_train.append(s)\n",
    "            y_train.append(l)\n",
    "\n",
    "    return (x_train, y_train), \\\n",
    "           (x_val, y_val), \\\n",
    "           (i2w, w2i), 2\n",
    "\n",
    "\n",
    "def gen_sentence(sent, g):\n",
    "\n",
    "    symb = '_[a-z]*'\n",
    "\n",
    "    while True:\n",
    "\n",
    "        match = re.search(symb, sent)\n",
    "        if match is None:\n",
    "            return sent\n",
    "\n",
    "        s = match.span()\n",
    "        sent = sent[:s[0]] + random.choice(g[sent[s[0]:s[1]]]) + sent[s[1]:]\n",
    "\n",
    "def gen_dyck(p):\n",
    "    open = 1\n",
    "    sent = '('\n",
    "    while open > 0:\n",
    "        if random.random() < p:\n",
    "            sent += '('\n",
    "            open += 1\n",
    "        else:\n",
    "            sent += ')'\n",
    "            open -= 1\n",
    "\n",
    "    return sent\n",
    "\n",
    "def gen_ndfa(p):\n",
    "\n",
    "    word = random.choice(['abc!', 'uvw!', 'klm!'])\n",
    "\n",
    "    s = ''\n",
    "    while True:\n",
    "        if random.random() < p:\n",
    "            return 's' + s + 's'\n",
    "        else:\n",
    "            s+= word\n",
    "\n",
    "def load_brackets(n=50_000, seed=0):\n",
    "    return load_toy(n, char=True, seed=seed, name='dyck')\n",
    "\n",
    "def load_ndfa(n=50_000, seed=0):\n",
    "    return load_toy(n, char=True, seed=seed, name='ndfa')\n",
    "\n",
    "def load_toy(n=50_000, char=True, seed=0, name='lang'):\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    if name == 'lang':\n",
    "        sent = '_s'\n",
    "\n",
    "        toy = {\n",
    "            '_s': ['_s _adv', '_np _vp', '_np _vp _prep _np', '_np _vp ( _prep _np )', '_np _vp _con _s' , '_np _vp ( _con _s )'],\n",
    "            '_adv': ['briefly', 'quickly', 'impatiently'],\n",
    "            '_np': ['a _noun', 'the _noun', 'a _adj _noun', 'the _adj _noun'],\n",
    "            '_prep': ['on', 'with', 'to'],\n",
    "            '_con' : ['while', 'but'],\n",
    "            '_noun': ['mouse', 'bunny', 'cat', 'dog', 'man', 'woman', 'person'],\n",
    "            '_vp': ['walked', 'walks', 'ran', 'runs', 'goes', 'went'],\n",
    "            '_adj': ['short', 'quick', 'busy', 'nice', 'gorgeous']\n",
    "        }\n",
    "\n",
    "        sentences = [ gen_sentence(sent, toy) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s : len(s))\n",
    "\n",
    "    elif name == 'dyck':\n",
    "\n",
    "        sentences = [gen_dyck(7./16.) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s: len(s))\n",
    "\n",
    "    elif name == 'ndfa':\n",
    "\n",
    "        sentences = [gen_ndfa(1./4.) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s: len(s))\n",
    "\n",
    "    else:\n",
    "        raise Exception(name)\n",
    "\n",
    "    tokens = set()\n",
    "    for s in sentences:\n",
    "\n",
    "        if char:\n",
    "            for c in s:\n",
    "                tokens.add(c)\n",
    "        else:\n",
    "            for w in s.split():\n",
    "                tokens.add(w)\n",
    "\n",
    "    i2t = [PAD, START, END, UNK] + list(tokens)\n",
    "    t2i = {t:i for i, t in enumerate(i2t)}\n",
    "\n",
    "    sequences = []\n",
    "    for s in sentences:\n",
    "        if char:\n",
    "            tok = list(s)\n",
    "        else:\n",
    "            tok = s.split()\n",
    "        sequences.append([t2i[t] for t in tok])\n",
    "\n",
    "    return sequences, (i2t, t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (i2w, w2i), numcls = load_imdb(final=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['possibly', 'the', 'best', 'movie', 'ever', 'created', 'in', 'the', 'history', 'of', 'jeffrey', 'combs', 'career', 'and', 'one', 'that', 'should', 'be', 'looked', 'upon', 'by', 'all', 'talent', 'in', 'hollywood', 'for', 'his', 'versatility', 'charisma', 'and', 'uniqueness', 'he', 'brings', 'through', 'his', 'characters', 'and', 'his', 'knowledge', 'of', 'acting']\n"
     ]
    }
   ],
   "source": [
    "print([i2w[w] for w in x_train[141]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(w2i['.pad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding and Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_and_conversion(batch_sequences, pad_index):\n",
    "    max_length = max(len(seq) for seq in batch_sequences)\n",
    "\n",
    "    padded_sequences= []\n",
    "    for seq in batch_sequences:\n",
    "        padded_seq = seq + [pad_index] * (max_length - len(seq))\n",
    "        padded_sequences.append(padded_seq)\n",
    "\n",
    "    # Conversion to a torch tensor\n",
    "    batch_tensor = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "    \n",
    "    return batch_tensor\n",
    "\n",
    "x_train = padding_and_conversion(x_train, pad_index=w2i[PAD])\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size=300, hidden_size=300, num_classes=2):\n",
    "        super(SequenceModel, self).__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
    "\n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(embedding_size, hidden_size)\n",
    "\n",
    "        # Global max pooling\n",
    "        self.global_max_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(inputs)\n",
    "\n",
    "        # Linear layer\n",
    "        linear_output = self.linear(embedded)\n",
    "\n",
    "        # ReLu activation function\n",
    "        activations = F.relu(linear_output)\n",
    "\n",
    "        # Max pooling\n",
    "        pooled_output = self.global_max_pooling(activations.transpose(1, 2)).squeeze(-1) \n",
    "\n",
    "        # Output layer\n",
    "        output = self.output_layer(pooled_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99430"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6833600401878357\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7240452766418457\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7718187570571899\n",
      "Accuracy: 0.3125\n",
      "Loss: 0.7070116996765137\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6973012685775757\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7502262592315674\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7556215524673462\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7134756445884705\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7710822224617004\n",
      "Accuracy: 0.375\n",
      "Loss: 0.6931874752044678\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.6986528635025024\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7403392195701599\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6633346676826477\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7281018495559692\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7401134967803955\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.719986617565155\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6804946064949036\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.6968882083892822\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6831931471824646\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.6975476741790771\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7167752981185913\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7533383965492249\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6893711090087891\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7450712323188782\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7560060620307922\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.723168671131134\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6561436057090759\n",
      "Accuracy: 0.6875\n",
      "Loss: 0.6847900748252869\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7271168828010559\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7471595406532288\n",
      "Accuracy: 0.375\n",
      "Loss: 0.78133225440979\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7103599905967712\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6939260363578796\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7309640645980835\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7131181359291077\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7161285877227783\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6624569296836853\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7079777121543884\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7133009433746338\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7174493670463562\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6868984699249268\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7483559250831604\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7097378969192505\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7249590754508972\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7467470169067383\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7171037197113037\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6757698655128479\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.6507654190063477\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.6429810523986816\n",
      "Accuracy: 0.71875\n",
      "Loss: 0.7183209657669067\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6732706427574158\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7286925315856934\n",
      "Accuracy: 0.3125\n",
      "Loss: 0.7037313580513\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6785112023353577\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7574875950813293\n",
      "Accuracy: 0.3125\n",
      "Loss: 0.7378497123718262\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7268713116645813\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7036269307136536\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7265666723251343\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6959398984909058\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6926586031913757\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7389582991600037\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7685542106628418\n",
      "Accuracy: 0.3125\n",
      "Loss: 0.6968789100646973\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.706671416759491\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6926940679550171\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.7667352557182312\n",
      "Accuracy: 0.28125\n",
      "Loss: 0.6874684691429138\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6637606620788574\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.7019027471542358\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.729835033416748\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7066412568092346\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7020213603973389\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7340649962425232\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7013944387435913\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7202330827713013\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6885836720466614\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6782881021499634\n",
      "Accuracy: 0.625\n",
      "Loss: 0.6951475143432617\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.735126256942749\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7013656497001648\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7137206196784973\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7234243154525757\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7675601840019226\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7335013151168823\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.6775654554367065\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.6891774535179138\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7176214456558228\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6963019371032715\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.668658435344696\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7014859318733215\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.719668447971344\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6893594264984131\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7101303935050964\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7251450419425964\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6686146855354309\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7174628973007202\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7250512838363647\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7068614959716797\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7231950163841248\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7175539135932922\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7174906730651855\n",
      "Accuracy: 0.28125\n",
      "Loss: 0.7242186665534973\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7010732293128967\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7475361227989197\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7401254177093506\n",
      "Accuracy: 0.375\n",
      "Loss: 0.6709061861038208\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7211571931838989\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6951228976249695\n",
      "Accuracy: 0.5\n",
      "Loss: 0.710995078086853\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.669475793838501\n",
      "Accuracy: 0.75\n",
      "Loss: 0.6639961004257202\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7707688808441162\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7032843232154846\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7446959614753723\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.6661720871925354\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.6885254383087158\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7436527609825134\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7329980731010437\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7055519223213196\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7024348378181458\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7421611547470093\n",
      "Accuracy: 0.3125\n",
      "Loss: 0.6876187920570374\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7064371705055237\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7283037900924683\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7435400485992432\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7207200527191162\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6914857625961304\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7021999955177307\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6666926145553589\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6973265409469604\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.718173086643219\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6979947090148926\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7128492593765259\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7633990049362183\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6965280771255493\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7338610887527466\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7390596866607666\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7289326786994934\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6894068717956543\n",
      "Accuracy: 0.625\n",
      "Loss: 0.7146940231323242\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6932560801506042\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7374004125595093\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7057860493659973\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6482062935829163\n",
      "Accuracy: 0.6875\n",
      "Loss: 0.7059061527252197\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7022338509559631\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7053388953208923\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7039766311645508\n",
      "Accuracy: 0.375\n",
      "Loss: 0.6985766887664795\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7090826034545898\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7085562348365784\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.715006411075592\n",
      "Accuracy: 0.5\n",
      "Loss: 0.74808669090271\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7015076279640198\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6917454600334167\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7509265542030334\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7619682550430298\n",
      "Accuracy: 0.28125\n",
      "Loss: 0.7057380080223083\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7136713862419128\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6904221773147583\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7419213652610779\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7419514656066895\n",
      "Accuracy: 0.3125\n",
      "Loss: 0.6996411085128784\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7593398094177246\n",
      "Accuracy: 0.25\n",
      "Loss: 0.7066905498504639\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7572240829467773\n",
      "Accuracy: 0.375\n",
      "Loss: 0.6775606274604797\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.735220193862915\n",
      "Accuracy: 0.3125\n",
      "Loss: 0.719444751739502\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.706139862537384\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7529627680778503\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7276595234870911\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7037174701690674\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7630400061607361\n",
      "Accuracy: 0.28125\n",
      "Loss: 0.6762776374816895\n",
      "Accuracy: 0.625\n",
      "Loss: 0.7286468148231506\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6930809020996094\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6949418783187866\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.6583741903305054\n",
      "Accuracy: 0.6875\n",
      "Loss: 0.6731534004211426\n",
      "Accuracy: 0.6875\n",
      "Loss: 0.7628160715103149\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7011909484863281\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6732205748558044\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7754339575767517\n",
      "Accuracy: 0.25\n",
      "Loss: 0.7061799168586731\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7231796979904175\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6618540287017822\n",
      "Accuracy: 0.6875\n",
      "Loss: 0.6735004782676697\n",
      "Accuracy: 0.625\n",
      "Loss: 0.6904419660568237\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6626734137535095\n",
      "Accuracy: 0.75\n",
      "Loss: 0.7205562591552734\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7095513343811035\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6846436858177185\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6771124005317688\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.7194328308105469\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7090910077095032\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7310095429420471\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6701059341430664\n",
      "Accuracy: 0.6875\n",
      "Loss: 0.6970706582069397\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7642576694488525\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7324756383895874\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7347928285598755\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6998183131217957\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6948259472846985\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.760021984577179\n",
      "Accuracy: 0.28125\n",
      "Loss: 0.7163058519363403\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6998730897903442\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.651447594165802\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.6854897737503052\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7430704236030579\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.703401505947113\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7112509608268738\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7102699279785156\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7155023813247681\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7276214361190796\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7251262068748474\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7217807173728943\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6965574622154236\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7145527005195618\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.718712568283081\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6749870181083679\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.6418265104293823\n",
      "Accuracy: 0.71875\n",
      "Loss: 0.7032249569892883\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6692198514938354\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7294777035713196\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7058234214782715\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7318329215049744\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6848328113555908\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7344058156013489\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7263520956039429\n",
      "Accuracy: 0.5\n",
      "Loss: 0.711234450340271\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6999858021736145\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7107887864112854\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7360196709632874\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7276486158370972\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7061463594436646\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7075391411781311\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7319197058677673\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7078053951263428\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.691246509552002\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7060582041740417\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7286850214004517\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7403655648231506\n",
      "Accuracy: 0.3125\n",
      "Loss: 0.6636257767677307\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.7084656357765198\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7050933837890625\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.699282705783844\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6538755297660828\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7092248797416687\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6832905411720276\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6922594308853149\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6570116281509399\n",
      "Accuracy: 0.6875\n",
      "Loss: 0.7203962802886963\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6990689039230347\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6450016498565674\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.7260466814041138\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7221938967704773\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.74395352602005\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7016370296478271\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7231333255767822\n",
      "Accuracy: 0.5\n",
      "Loss: 0.702292799949646\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7264033555984497\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7372626662254333\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7078995704650879\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7144374251365662\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7481796145439148\n",
      "Accuracy: 0.375\n",
      "Loss: 0.6750472187995911\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.7029030323028564\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7022703886032104\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7454743981361389\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7314861416816711\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6930586099624634\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7039660215377808\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6912606954574585\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7304232716560364\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7121152877807617\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6861677765846252\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6998145580291748\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6989017128944397\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6839153170585632\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7227329015731812\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.688157856464386\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7373387813568115\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7383010387420654\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.6867298483848572\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.749330997467041\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7451828718185425\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7478465437889099\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7197085618972778\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7626572251319885\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7126308679580688\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6906092762947083\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7063955664634705\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7705379724502563\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.6994500160217285\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7135022878646851\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6854739189147949\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.6999014019966125\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.686537504196167\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6635861992835999\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.6548510789871216\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.7079759836196899\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7540250420570374\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7190475463867188\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7355976700782776\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7334327101707458\n",
      "Accuracy: 0.375\n",
      "Loss: 0.6966482400894165\n",
      "Accuracy: 0.625\n",
      "Loss: 0.7009078860282898\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6778368353843689\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.6917306780815125\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6974853873252869\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7121097445487976\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7143100500106812\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6941698789596558\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7280529141426086\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7624186277389526\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7439228892326355\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7362678647041321\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7322328090667725\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7109341025352478\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6829084753990173\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7294466495513916\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7038719058036804\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6817473769187927\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7359917163848877\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7285235524177551\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7023520469665527\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.710486650466919\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7208837866783142\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7021218538284302\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7148842811584473\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7383551597595215\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6907843947410583\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7359910607337952\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6833038926124573\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7350258827209473\n",
      "Accuracy: 0.375\n",
      "Loss: 0.6867223381996155\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.703912079334259\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7127400040626526\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6836166977882385\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.6811854839324951\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7475584149360657\n",
      "Accuracy: 0.3125\n",
      "Loss: 0.6958944201469421\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6914718747138977\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7279694676399231\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7071258425712585\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7147160768508911\n",
      "Accuracy: 0.375\n",
      "Loss: 0.6978663206100464\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7209275960922241\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7167572975158691\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7065076231956482\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7494239211082458\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6989741325378418\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6856176853179932\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7161157727241516\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7461267113685608\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.689268171787262\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7250837683677673\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7072681188583374\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7353318929672241\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6460683941841125\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.6795089244842529\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6818844676017761\n",
      "Accuracy: 0.625\n",
      "Loss: 0.7044830322265625\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7492231726646423\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7241849303245544\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6899275183677673\n",
      "Accuracy: 0.625\n",
      "Loss: 0.6783539652824402\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7312095165252686\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7087002396583557\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7167505025863647\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.686405599117279\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6777364611625671\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.66942298412323\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7299422025680542\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.6952657103538513\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7175823450088501\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6378108859062195\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.7409396767616272\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7194316387176514\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7400931715965271\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.695396363735199\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7173576354980469\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7714477181434631\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7093538045883179\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7252973318099976\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7036574482917786\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.683380663394928\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7296208739280701\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6906830668449402\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6882762908935547\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7333223819732666\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7141455411911011\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6929181814193726\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.706137478351593\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.72251957654953\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7030856609344482\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.711588978767395\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6866503357887268\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.7227147817611694\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7123075723648071\n",
      "Accuracy: 0.5\n",
      "Loss: 0.751068115234375\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7544992566108704\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7049402594566345\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6876555681228638\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7206100225448608\n",
      "Accuracy: 0.5\n",
      "Loss: 0.720718502998352\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7579545378684998\n",
      "Accuracy: 0.3125\n",
      "Loss: 0.7476001977920532\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7137693762779236\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7260578274726868\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7126458883285522\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7211790084838867\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7224805355072021\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7539642453193665\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6746151447296143\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7210190296173096\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.6884857416152954\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7311283946037292\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7116081714630127\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7357507348060608\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7047107219696045\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6496797800064087\n",
      "Accuracy: 0.71875\n",
      "Loss: 0.7186412811279297\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6503776907920837\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.7075651288032532\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7318092584609985\n",
      "Accuracy: 0.375\n",
      "Loss: 0.6778959631919861\n",
      "Accuracy: 0.71875\n",
      "Loss: 0.7575545310974121\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7379356622695923\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7123368978500366\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6794928312301636\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.75618976354599\n",
      "Accuracy: 0.375\n",
      "Loss: 0.6913890838623047\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7475759387016296\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.6589992642402649\n",
      "Accuracy: 0.6875\n",
      "Loss: 0.678308367729187\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7349459528923035\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7084788680076599\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7397121787071228\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6891369223594666\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7113907933235168\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7053836584091187\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.6927735209465027\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7435514330863953\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7278003096580505\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7268481850624084\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7243623733520508\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6999267935752869\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7198348045349121\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7311951518058777\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7411190867424011\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6685430407524109\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7130720615386963\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7024791240692139\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.728891134262085\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7209708094596863\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7775605320930481\n",
      "Accuracy: 0.28125\n",
      "Loss: 0.7469282150268555\n",
      "Accuracy: 0.375\n",
      "Loss: 0.745201587677002\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6875281929969788\n",
      "Accuracy: 0.625\n",
      "Loss: 0.6887016892433167\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6963435411453247\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.691249668598175\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7439960241317749\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6760483384132385\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7273699045181274\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6943137049674988\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7675173282623291\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.704264760017395\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6967220902442932\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7213016152381897\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7424958944320679\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.67582768201828\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6719474196434021\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6971246600151062\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6439471244812012\n",
      "Accuracy: 0.6875\n",
      "Loss: 0.6875042915344238\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7224264144897461\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7382742762565613\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7356355786323547\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7334781885147095\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7210526466369629\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7116902470588684\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6669707298278809\n",
      "Accuracy: 0.625\n",
      "Loss: 0.7231062054634094\n",
      "Accuracy: 0.375\n",
      "Loss: 0.6967747807502747\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6817358136177063\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7270882725715637\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6890063881874084\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7011841535568237\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7422338724136353\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7229319214820862\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7236658334732056\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7317514419555664\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.6914570927619934\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6995915770530701\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7245767712593079\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.713513970375061\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6788352727890015\n",
      "Accuracy: 0.625\n",
      "Loss: 0.7193728685379028\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.72059565782547\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7026267051696777\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.741582453250885\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7546342611312866\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7199030518531799\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7352880835533142\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6955594420433044\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7611802816390991\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7196568250656128\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7065248489379883\n",
      "Accuracy: 0.5\n",
      "Loss: 0.743474006652832\n",
      "Accuracy: 0.3125\n",
      "Loss: 0.7266028523445129\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7407020330429077\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7234309911727905\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7524258494377136\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7038400173187256\n",
      "Accuracy: 0.5\n",
      "Loss: 0.707874596118927\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7140990495681763\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7533063888549805\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7934669256210327\n",
      "Accuracy: 0.3125\n",
      "Loss: 0.6956207752227783\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7083688378334045\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7252922058105469\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7143328785896301\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6751300096511841\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.6826587319374084\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.714876115322113\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6727012395858765\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7112914323806763\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.66776442527771\n",
      "Accuracy: 0.625\n",
      "Loss: 0.7376293540000916\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6903996467590332\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6912222504615784\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7152577638626099\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6713115572929382\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.6683909296989441\n",
      "Accuracy: 0.625\n",
      "Loss: 0.6951990127563477\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7281897068023682\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6965402960777283\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6846341490745544\n",
      "Accuracy: 0.625\n",
      "Loss: 0.6909905672073364\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7133883833885193\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.752818763256073\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7359750866889954\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7784810662269592\n",
      "Accuracy: 0.21875\n",
      "Loss: 0.736449658870697\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7362305521965027\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7328797578811646\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7091684937477112\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6783869862556458\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7532376050949097\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7685421705245972\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7115527391433716\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7266243696212769\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7220185995101929\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7336378693580627\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7799310088157654\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7416485548019409\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6887943744659424\n",
      "Accuracy: 0.625\n",
      "Loss: 0.6847026944160461\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.694191575050354\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7299894094467163\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.7020936012268066\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6729986071586609\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6891370415687561\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6964935064315796\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7434636354446411\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7298744916915894\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6939474940299988\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7373335361480713\n",
      "Accuracy: 0.34375\n",
      "Loss: 0.70501708984375\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7321210503578186\n",
      "Accuracy: 0.5\n",
      "Loss: 0.702332079410553\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7039982676506042\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6961671113967896\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7113220691680908\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.718909740447998\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.6959313750267029\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7171379327774048\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7217759490013123\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7345553040504456\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7432990074157715\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6945749521255493\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6905459761619568\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7548547387123108\n",
      "Accuracy: 0.375\n",
      "Loss: 0.6991504430770874\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6636056303977966\n",
      "Accuracy: 0.625\n",
      "Loss: 0.7280082702636719\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.699137270450592\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7050556540489197\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.6886035203933716\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7017895579338074\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7125971913337708\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7087066173553467\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6730070114135742\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.6639378666877747\n",
      "Accuracy: 0.625\n",
      "Loss: 0.667367160320282\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7559671401977539\n",
      "Accuracy: 0.375\n",
      "Loss: 0.6832910180091858\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.7101929783821106\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7063889503479004\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.6876925826072693\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.7247208952903748\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.7327019572257996\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6934390068054199\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7323120832443237\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.6942756175994873\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6876991391181946\n",
      "Accuracy: 0.53125\n",
      "Loss: 0.7205165028572083\n",
      "Accuracy: 0.5\n",
      "Loss: 0.747305691242218\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6660541296005249\n",
      "Accuracy: 0.625\n",
      "Loss: 0.7564640641212463\n",
      "Accuracy: 0.375\n",
      "Loss: 0.7122049927711487\n",
      "Accuracy: 0.59375\n",
      "Loss: 0.691909909248352\n",
      "Accuracy: 0.5625\n",
      "Loss: 0.7013728618621826\n",
      "Accuracy: 0.46875\n",
      "Loss: 0.7014155983924866\n",
      "Accuracy: 0.5\n",
      "Loss: 0.6518545746803284\n",
      "Accuracy: 0.625\n",
      "Loss: 0.7005866169929504\n",
      "Accuracy: 0.65625\n",
      "Loss: 0.7375866174697876\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.708621621131897\n",
      "Accuracy: 0.40625\n",
      "Loss: 0.7081067562103271\n",
      "Accuracy: 0.4375\n",
      "Loss: 0.6992245316505432\n",
      "Accuracy: 0.5\n",
      "Loss: 0.7042830586433411\n",
      "Accuracy: 0.59375\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(w2i)\n",
    "model = SequenceModel(vocab_size)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training Loop\n",
    "for inputs, labels in train_loader:\n",
    "    output = model(inputs)\n",
    "\n",
    "    loss = F.cross_entropy(output, labels)\n",
    "    \n",
    "    predicted_labels = torch.argmax(output, dim=1)\n",
    "    accuracy = (predicted_labels == labels).float().mean()\n",
    "\n",
    "    print(\"Loss:\", loss.item())\n",
    "    print(\"Accuracy:\", accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
